name: üß™ Maximum Test (10 Items)

on:
  workflow_dispatch:

jobs:
  test-scrape-python:
    name: üêç Test Scrape (Limit 10)
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # 1. Setup Python Environment
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install Python Dependencies
        run: |
          pip install -r requirements.txt
          google-chrome --version || true

      # 2. Setup Node.js
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Node Dependencies
        run: npm ci

      # 3. Run Python Scraper with Limit 10 (Headful via Xvfb)
      - name: üöÄ Run Python Scraper (Limit 10)
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb
          # Pass --limit=10 to the python script
          xvfb-run --auto-servernum --server-args="-screen 0 1280x1024x24" python3 -u src/scrapers/isbankasi/maximum.py --limit=10
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      # 4. Import to Supabase
      - name: üíæ Import to Supabase
        run: npm run import:maximum
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      # 5. Post-Process
      - name: üîß Post-Process
        if: always()
        run: |
          npm run fix:brands
          npx tsx src/garbageCollector.ts
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
